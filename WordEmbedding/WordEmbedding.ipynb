{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirPpath = Path(os.path.abspath('')).parent # Fetching the current directory path - Specific for ipynb file - For .py: Path(os.path.dirname(os.path.realpath(__file__)).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "PledgesCsvPath = str(DirPpath.absolute()) + \"/CleanedData.csv\"  \n",
    "\n",
    "PledgesDf = pd.read_csv(PledgesCsvPath, index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ecaudron001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ecaudron001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ecaudron001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ecaudron001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Working on a monitoring scheme for climate-neutral destinations. Calculating and monitoring the (real! comprehensive) CO2-Footprint of tourism; developing programs to avoid, reduce and compensate. The methodology should be ready, tested and available by end of 2023.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def first_clean(text):\n",
    "    return \" \".join(text.split()).replace(\"_x000D_\",\"\")\n",
    "\n",
    "first_clean(PledgesDf.iloc[10,1]) # Delete the returns from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".The Catalan Tourism Data System platform (TDS): is an electronic platform of the online survey system that allows managing and analyzing the evolution of the activity of accommodation and comparing it with the global destination, the type of accommodation and other brands tourist. The target for the next years would be to include expenditure data at the regional level\\n•\\thttps://empresa.gencat.cat/ca/treb_ambits_actuacio/turisme/coneixement_planificacio/estadistiques-turistiques/tourism-data-system\\n•\\thttps://catalunya.tourism-data-system.cat/\\n\\n.The system of indicators of the Tourism Intelligence Indicators System (INTUCAT) has been approved. This index is structured based on 4 areas of measurement, which refer to the environmental sustainability, social sustainability and economic sustainability of the tourism activity at the destination. A fourth area that has been considered essential is the measurement of the achievement of the Sustainable Development Goals (SDGs).\\nTarget: to develop and implement the System INTUCAT \\n•\\thttps://empresa.gencat.cat/ca/treb_ambits_actuacio/turisme/coneixement_planificacio/indicadors-turistics-ictucat-intucat/\\n\\n. INTEL·LITUR: \\nWith a clear orientation towards data culture in the companies, Catalonia is promoting a platform where the client of the Catalan Tourist Board (CTB) (companies in the Catalan tourism sector) can find all the key information they need regarding products and markets. \\nThe platform, within the B2B portal of the CTB, consists of two parts, one part with quantitative information in PowerBI format and a virtual library with documents of interest generated by the CTB.\\nThe information is structured according to the life cycle of the trip or according to the needs of the company: attracting customers, gaining customer loyalty, increasing productivity, increasing profitability or maintaining the company's staff.\\nTarget: the first phase should be online by the end of 2023 (quantitative data), the second one by the end of 2024 (virtual library).\\n\\n. TOT LAB: \\nCatalonia is part of TOTLAB, NECSTOUR's knowledge laboratory.\\nWe are working this 2022 on a project of indicators of sustainability in the territory focused on the so-called 4D, diversification of the tourist offer, territorial deconcentration of tourism, deseasonalization of the arrival of tourist flows and spending (Despesa in Catalan).\\nThe aim of this work is to establish an index for measuring the correct implementation of the wise growth strategy proposed in our Marketing plan 18-23.\\nTarget: Each year we will work on a topic that can be shared with the other participating members of TOTLAB, and so the other members should do.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ReplaceContractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "ReplaceContractions(PledgesDf.iloc[9,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Achieve sustainable and flexible solutions for multimodal transport and develop policies to protect natural heritage and biodiversity, respecting the socio-cultural authenticity of host communities.\\n\\nCNA Turismo e Commercio, over the three-year period (between autumn 2022 and throughout 2023), will organize training seminars for businesses - with the involvement of public and private stakeholders - aimed at the implementation of concrete solutions for the development of good practices for a supply of multimodal transport and protocols for the respect and protection of the natural heritage and biodiversity.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RemoveURL(text):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    text = re.sub(r\"https:\\s?\\S+\", \"\", text)\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "RemoveURL(PledgesDf.iloc[5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actually we as an association are still pretty much at the beginning due to the pandemic which took the better part of our ressources what we want to provide is proper guideline for str how to achieve maintain and develop sustainable business additionally our business is very fragmented and diverse we have privat hosts with one or only few apartments professional hosts local property managers of all sizes platforms for str local national and international level and service providers for the industry of all kind our target for is to achieve that guideline nonetheless some of our members have implemented variation of measures like strategies for sustainable transmission climate compensation specific advertising for sustainable accomodations using local supplies charging stations for cars using renewable energy and co neutral construction tools to minimise negative impact of tourism on neighbourhoods etc'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower() # Lowercase all the characters from the string\n",
    "    text = text.strip() # Remove the leading and trailing whitespaces\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text) # Removing Punctuation\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text)) # Remove non alphanumeric characters\n",
    "    text = re.sub(r'\\d', '', text) # Removing digits\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text) # Removing single characters\n",
    "    text = re.sub(r'\\s+', ' ', str(text).strip()) # Replacing \"double, triple, etc\" whitespaces by one\n",
    "    return text\n",
    "\n",
    "preprocess(ReplaceContractions(RemoveURL(first_clean(PledgesDf.iloc[0,1])))) # Testing on pledge 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'estonia sees need synchronize harmonize rules short term rentals eu level participate process common development implementation common principles however bearing mind regions different may need different approach'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stopword(string):\n",
    "    a = [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "stopword(preprocess(ReplaceContractions(RemoveURL(first_clean(PledgesDf.iloc[3,1])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actually association still pretty much begin due pandemic take well part ressources want provide proper guideline str achieve maintain develop sustainable business additionally business fragment diverse privat host one apartment professional host local property manager size platform str local national international level service provider industry kind target achieve guideline nonetheless member implement variation measure like strategy sustainable transmission climate compensation specific advertising sustainable accomodations use local supply charge station car use renewable energy co neutral construction tool minimise negative impact tourism neighbourhood etc'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(nltk.word_tokenize(string))  # Get position tags\n",
    "    a = [wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in\n",
    "         enumerate(word_pos_tags)]  # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "lemmatizer(stopword(preprocess(first_clean(PledgesDf.iloc[0,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy # python -m spacy download en_core_web_sm = CMD in your terminal\n",
    "\n",
    "PledgesDf['PreProcessedText'] = PledgesDf['Pledge'].apply(lambda x: preprocess(stopword(ReplaceContractions(RemoveURL(first_clean(x))))))\n",
    "data_words = [nltk.word_tokenize(i) for i in PledgesDf['PreProcessedText']]\n",
    "\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stopwords.words('english'), allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return [' '.join(pledge) for pledge in texts_out]\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "\n",
    "    global n\n",
    "    n = n +1\n",
    "    \n",
    "    print(\"**************\")\n",
    "    print(\"n is : \")\n",
    "    print(n)\n",
    "    print(\"length of the text is : \")\n",
    "    print(len(first_clean(text)))\n",
    "    return lemmatizer(preprocess(stopword(ReplaceContractions(RemoveURL(first_clean(text))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "print(\"begin pre-processing\")\n",
    "PledgesDf['clean_text'] = PledgesDf['Pledge'].apply(lambda x: preprocessing(x))\n",
    "print(\"end pre-processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "PledgesDf['clean_text2'] = data_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Pledge</th>\n",
       "      <th>Pledge status</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Actually we as an association are still pretty...</td>\n",
       "      <td>Published</td>\n",
       "      <td>actually association still pretty much begin d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>EFFAT welcomes the Commission Proposal for a R...</td>\n",
       "      <td>Ready for publishing</td>\n",
       "      <td>effat welcome commission proposal regulation d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>HOTREC calls for a level playing field and fai...</td>\n",
       "      <td>Published</td>\n",
       "      <td>hotrec call level play field fair competition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Estonia sees the need to synchronize and harmo...</td>\n",
       "      <td>Published</td>\n",
       "      <td>estonia see need synchronize harmonize rule sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Sphere Travel Club contributes to a flourishin...</td>\n",
       "      <td>Ready for publishing</td>\n",
       "      <td>sphere travel club contribute flourish transpa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>28</td>\n",
       "      <td>Digital Innovation of Cultural Heritage (DICH)...</td>\n",
       "      <td>Published</td>\n",
       "      <td>digital innovation cultural heritage dich over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>28</td>\n",
       "      <td>“Lofoten the Green Islands 2030” is a programm...</td>\n",
       "      <td>Published</td>\n",
       "      <td>lofoten green island programme partnership rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>28</td>\n",
       "      <td>To promote health tourism as the unique sellin...</td>\n",
       "      <td>Ready for publishing</td>\n",
       "      <td>to promote health tourism unique selling point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>28</td>\n",
       "      <td>As a small tour operator we try to engage our ...</td>\n",
       "      <td>Published</td>\n",
       "      <td>a small tour operator try engage customer make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>28</td>\n",
       "      <td>Regional strategy for sustainable development ...</td>\n",
       "      <td>Published</td>\n",
       "      <td>regional strategy sustainable development appr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>373 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic                                             Pledge  \\\n",
       "0        1  Actually we as an association are still pretty...   \n",
       "1        1  EFFAT welcomes the Commission Proposal for a R...   \n",
       "2        1  HOTREC calls for a level playing field and fai...   \n",
       "3        1  Estonia sees the need to synchronize and harmo...   \n",
       "4        1  Sphere Travel Club contributes to a flourishin...   \n",
       "..     ...                                                ...   \n",
       "377     28  Digital Innovation of Cultural Heritage (DICH)...   \n",
       "378     28  “Lofoten the Green Islands 2030” is a programm...   \n",
       "379     28  To promote health tourism as the unique sellin...   \n",
       "380     28  As a small tour operator we try to engage our ...   \n",
       "381     28  Regional strategy for sustainable development ...   \n",
       "\n",
       "            Pledge status                                         clean_text  \n",
       "0               Published  actually association still pretty much begin d...  \n",
       "1    Ready for publishing  effat welcome commission proposal regulation d...  \n",
       "2               Published  hotrec call level play field fair competition ...  \n",
       "3               Published  estonia see need synchronize harmonize rule sh...  \n",
       "4    Ready for publishing  sphere travel club contribute flourish transpa...  \n",
       "..                    ...                                                ...  \n",
       "377             Published  digital innovation cultural heritage dich over...  \n",
       "378             Published  lofoten green island programme partnership rea...  \n",
       "379  Ready for publishing  to promote health tourism unique selling point...  \n",
       "380             Published  a small tour operator try engage customer make...  \n",
       "381             Published  regional strategy sustainable development appr...  \n",
       "\n",
       "[373 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PledgesDf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\" \".join(PledgesDf['clean_text']).split()).value_counts()[:30].index.tolist() # Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords2 = pd.Series(\" \".join(PledgesDf['clean_text']).split()).value_counts()[:30].index.tolist()\n",
    "\n",
    "def RemoveFrequentWords(string, FrequentWords):\n",
    "    a = [i for i in string.split() if i not in FrequentWords] # Removing usual english stopwords from the string\n",
    "    return ' '.join(a) #Output - Same string after all the transformations\n",
    "\n",
    "PledgesDf['clean_text'] = PledgesDf['clean_text'].apply(lambda x: RemoveFrequentWords(x, StopWords2))\n",
    "\n",
    "print(PledgesDf.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(i) for i in PledgesDf[\"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3209"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = {}\n",
    "for sent in tokens:\n",
    "    for i in sent:\n",
    "\n",
    "        if i not in word_freq.keys():\n",
    "            word_freq[i] = 1\n",
    "        else:\n",
    "            word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word')\n",
    "\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(PledgesDf[\"clean_text\"])\n",
    "\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names_out()\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)\n",
    "\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2 = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Word2Vec model with Average method\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "            return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "w2v2 = dict(zip(wv2.index_to_key, wv2.vectors))\n",
    "modelw2 = MeanEmbeddingVectorizer(w2v2)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "vectors_w2v2 = modelw2.transform(tokens)\n",
    "\n",
    "print(vectors_w2v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocIndexV1 = pd.DataFrame(vectors_w2v2)\n",
    "IndexedPath = str(DirPpath.absolute()) + \"\\IndexedDataV1.csv\"\n",
    "DocIndexV1.to_csv(IndexedPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Word2Vec model with TF-IDF method\n",
    "class TfIdfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "            return self\n",
    "\n",
    "    def transform(self, X, tfidf):\n",
    "\n",
    "        DocList = []\n",
    "        i = 0\n",
    "        \n",
    "        for words in X:\n",
    "\n",
    "            WordList = []\n",
    "            SumWeight = 0 # For applying a sum product appoach\n",
    "\n",
    "            for w in words:\n",
    "                 \n",
    "                try:\n",
    "                    if w in self.word2vec:\n",
    "                        weight = tfidf[w].iloc[i]\n",
    "                        WordList.append(self.word2vec[w] * weight)\n",
    "                    else:\n",
    "                        weight = tfidf[w].iloc[i]\n",
    "                        WordList.append(np.zeros(self.dim))\n",
    "                except:\n",
    "                    weight = 0\n",
    "                    WordList.append(np.zeros(self.dim))\n",
    "\n",
    "                SumWeight += weight\n",
    "\n",
    "            i+=1\n",
    "            DocList.append(np.sum(np.array(WordList), axis = 0) / SumWeight) \n",
    "        \n",
    "        print(np.array(DocList).shape)\n",
    "        return np.array(DocList)\n",
    "\n",
    "\n",
    "w2v2 = dict(zip(wv2.index_to_key, wv2.vectors))\n",
    "modelw2 = TfIdfEmbeddingVectorizer(w2v2)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "vectors_w2v2 = modelw2.transform(tokens, df_tfidfvect)\n",
    "\n",
    "print(vectors_w2v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocIndexV1 = pd.DataFrame(vectors_w2v2)\n",
    "IndexedPath = str(DirPpath.absolute()) + \"\\IndexedDataV1Tf.csv\"\n",
    "DocIndexV1.to_csv(IndexedPath)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Google 300 news Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Setting the model with its hyperparameters\n",
    "#model = Word2Vec(sentences = tokens, vector_size=300, min_count = 1)\n",
    "model2 = Word2Vec(sentences = tokens, vector_size=300, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(r\"C:\\Users\\ecaudron001\\Downloads\\GoogleNews-vectors-negative300.bin\",\n",
    "                                         binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.build_vocab(tokens)\n",
    "total_examples = model2.corpus_count\n",
    "\n",
    "vocab = list(model2.wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.build_vocab([list(model.key_to_index.keys())], update=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datascience.stackexchange.com/questions/97568/fine-tuning-pre-trained-word2vec-model-with-gensim-4-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.vectors_lockf = np.ones(len(model2.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.intersect_word2vec_format(r\"C:\\Users\\ecaudron001\\Downloads\\GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115070, 115070)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.train(tokens, total_examples=total_examples, epochs=model2.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Call', 0.6494125723838806),\n",
       " ('calls', 0.6452654004096985),\n",
       " ('calling', 0.6199396252632141),\n",
       " ('dial_#-###-###', 0.582241415977478),\n",
       " ('visit_www.clairmail.com', 0.5597918629646301),\n",
       " ('visit_www.oscars.org', 0.5302615761756897),\n",
       " ('See_www.thewerehouse.com', 0.5248600244522095),\n",
       " ('Charlene_Pellin_answered', 0.5245307683944702),\n",
       " ('information_visit_http://www.tempurpedic.com', 0.5230315327644348),\n",
       " ('calll', 0.521233081817627)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar(\"call\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(str(DirPpath.absolute()) + \"\\Word2VecFt.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.91528341e-02  2.45144889e-02 -9.77118686e-03 ... -2.14853287e-02\n",
      "  -5.02708368e-03 -1.71505511e-02]\n",
      " [-1.31707042e-02 -2.82868720e-03  3.06689576e-03 ... -2.37877597e-03\n",
      "   2.52336599e-02 -2.39090733e-02]\n",
      " [-2.01607645e-02  2.33879872e-02  9.59545281e-03 ... -3.29050049e-02\n",
      "   3.04455496e-02  1.77179743e-02]\n",
      " ...\n",
      " [-4.74511497e-02  2.40889918e-02 -2.04083859e-03 ... -3.03844269e-02\n",
      "   6.61780164e-02 -7.16137365e-05]\n",
      " [-1.23493625e-02  1.32553726e-02 -4.54793079e-03 ... -2.89725866e-02\n",
      "   8.95279124e-02 -1.37805715e-02]\n",
      " [-3.62874232e-02  1.73829589e-03 -1.23673230e-02 ... -1.07643884e-02\n",
      "   2.47869939e-02  1.84420142e-02]]\n"
     ]
    }
   ],
   "source": [
    "w2v2 = dict(zip(model2.wv.index_to_key, model2.wv.vectors))\n",
    "modelw2 = MeanEmbeddingVectorizer(w2v2)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "vectors_w2v2 = modelw2.transform(tokens)\n",
    "\n",
    "print(vectors_w2v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocIndexV1 = pd.DataFrame(vectors_w2v2)\n",
    "IndexedPath = str(DirPpath.absolute()) + \"\\IndexedDataV2.csv\"\n",
    "DocIndexV1.to_csv(IndexedPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252, 300)\n",
      "[[-0.3396613   1.26389937 -1.31796261 ... -0.53777768 -1.22251308\n",
      "   1.30233548]\n",
      " [-0.17496285  0.07041772 -0.02574716 ... -0.11364882  0.13680986\n",
      "  -0.14311348]\n",
      " [-0.02132123  0.07667647  0.0689479  ... -0.25388294  0.22600214\n",
      "   0.17822382]\n",
      " ...\n",
      " [-0.83201844  0.54158151  0.26405451 ... -0.27425581  0.83317399\n",
      "   0.05091583]\n",
      " [ 0.03621177  0.34011728 -0.16661881 ... -0.88733387  3.00252008\n",
      "   0.23049708]\n",
      " [-0.4722026  -0.00477884 -0.30624428 ...  0.10495146  0.26195163\n",
      "   0.27870798]]\n"
     ]
    }
   ],
   "source": [
    "w2v2 = dict(zip(model2.wv.index_to_key, model2.wv.vectors))\n",
    "modelw2 = TfIdfEmbeddingVectorizer(w2v2)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "vectors_w2v2 = modelw2.transform(tokens, df_tfidfvect)\n",
    "\n",
    "print(vectors_w2v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocIndexV1 = pd.DataFrame(vectors_w2v2)\n",
    "IndexedPath = str(DirPpath.absolute()) + \"\\IndexedDataV2Tf.csv\"\n",
    "DocIndexV1.to_csv(IndexedPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
