{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do later: Fetching data file automatically\n",
    "DirPpath = Path(os.path.abspath('')).parent # Fetching the current directory path - Specific for ipynb file - For .py: Path(os.path.dirname(os.path.realpath(__file__)).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "PledgesCsvPath = str(DirPpath.absolute()) + \"/DataExploration/CleanedData.csv\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PledgesDf = pd.read_csv(PledgesCsvPath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Achieve sustainable and flexible solutions for multimodal transport and develop policies to protect natural heritage and biodiversity, respecting the socio-cultural authenticity of host communities.  CNA Turismo e Commercio, over the three-year period (between autumn 2022 and throughout 2023), will organize training seminars for businesses - with the involvement of public and private stakeholders - aimed at the implementation of concrete solutions for the development of good practices for a supply of multimodal transport and protocols for the respect and protection of the natural heritage and biodiversity.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def first_clean(text):\n",
    "    return \" \".join(text.split()).replace(\"_x000D_\",\"\")\n",
    "\n",
    "first_clean(PledgesDf.iloc[3,1]) # Delete the returns from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'achieve sustainable and flexible solutions for multimodal transport and develop policies to protect natural heritage and biodiversity respecting the socio cultural authenticity of host communities cna turismo e commercio over the three year period between autumn and throughout will organize training seminars for businesses with the involvement of public and private stakeholders aimed at the implementation of concrete solutions for the development of good practices for a supply of multimodal transport and protocols for the respect and protection of the natural heritage and biodiversity'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower() # Lowercase all the characters from the string\n",
    "    text = text.strip() # Remove the leading and trailing whitespaces\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text) # Removing Punctuation\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text)) # Remove non alphanumeric characters\n",
    "    text = re.sub(r'\\d', '', text) # Removing digits\n",
    "    text = re.sub(r'\\s+', ' ', str(text).strip()) # Replacing \"double, triple, etc\" whitespaces by one\n",
    "    return text\n",
    "\n",
    "preprocess(first_clean(PledgesDf.iloc[3,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'achieve sustainable flexible solutions multimodal transport develop policies protect natural heritage biodiversity respecting socio cultural authenticity host communities cna turismo e commercio three year period autumn throughout organize training seminars businesses involvement public private stakeholders aimed implementation concrete solutions development good practices supply multimodal transport protocols respect protection natural heritage biodiversity'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stopword(string):\n",
    "    a = [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "stopword(preprocess(first_clean(PledgesDf.iloc[3,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'achieve sustainable flexible solution multimodal transport develop policy protect natural heritage biodiversity respect socio cultural authenticity host community cna turismo e commercio three year period autumn throughout organize training seminar business involvement public private stakeholder aim implementation concrete solution development good practice supply multimodal transport protocol respect protection natural heritage biodiversity'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(nltk.word_tokenize(string))  # Get position tags\n",
    "    a = [wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in\n",
    "         enumerate(word_pos_tags)]  # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "lemmatizer(stopword(preprocess(first_clean(PledgesDf.iloc[3,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "\n",
    "    global n\n",
    "    n = n +1\n",
    "    \n",
    "    print(\"**************\")\n",
    "    print(\"n is : \")\n",
    "    print(n)\n",
    "    print(\"length of the text is : \")\n",
    "    print(len(first_clean(text)))\n",
    "    return lemmatizer(stopword(preprocess(first_clean(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "print(\"begin pre-processing\")\n",
    "PledgesDf['clean_text'] = PledgesDf['Pledge'].apply(lambda x: preprocessing(x))\n",
    "print(\"end pre-processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Pledge</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>HOTREC calls for a level playing field and fai...</td>\n",
       "      <td>hotrec call level play field fair competition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Actually we as an association are still pretty...</td>\n",
       "      <td>actually association still pretty much begin d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Estonia sees the need to synchronize and harmo...</td>\n",
       "      <td>estonia see need synchronize harmonize rule sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Achieve sustainable and flexible solutions for...</td>\n",
       "      <td>achieve sustainable flexible solution multimod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>The Ile-de-France Region aims to make tourist ...</td>\n",
       "      <td>ile de france region aim make tourist site acc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic                                             Pledge  \\\n",
       "0      1  HOTREC calls for a level playing field and fai...   \n",
       "1      1  Actually we as an association are still pretty...   \n",
       "2      1  Estonia sees the need to synchronize and harmo...   \n",
       "3      2  Achieve sustainable and flexible solutions for...   \n",
       "4      2  The Ile-de-France Region aims to make tourist ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  hotrec call level play field fair competition ...  \n",
       "1  actually association still pretty much begin d...  \n",
       "2  estonia see need synchronize harmonize rule sh...  \n",
       "3  achieve sustainable flexible solution multimod...  \n",
       "4  ile de france region aim make tourist site acc...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PledgesDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(i) for i in PledgesDf[\"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3853"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = {}\n",
    "for sent in tokens:\n",
    "    for i in sent:\n",
    "\n",
    "        if i not in word_freq.keys():\n",
    "            word_freq[i] = 1\n",
    "        else:\n",
    "            word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tourism',\n",
       " 'sustainable',\n",
       " 'destination',\n",
       " 'de',\n",
       " 'development',\n",
       " 'data',\n",
       " 'project',\n",
       " 'support',\n",
       " 'sustainability',\n",
       " 'sector']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model with its hyperparameters\n",
    "model = Word2Vec(sentences = tokens, vector_size=100, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03200323,  0.04947015,  0.0303732 ,  0.01229963, -0.00495085,\n",
       "       -0.08394027,  0.02444905,  0.09911132, -0.02422472, -0.00613349,\n",
       "       -0.02879979, -0.05694728, -0.00317937,  0.0193244 , -0.01179363,\n",
       "       -0.05869448, -0.00354247, -0.08079367, -0.03086811, -0.06268363,\n",
       "        0.03832952,  0.03726424,  0.04147314, -0.02776401, -0.01850415,\n",
       "       -0.00827994, -0.04463762, -0.03819501, -0.01409421,  0.01621452,\n",
       "        0.05203019,  0.00444728,  0.01470546, -0.001519  , -0.02608609,\n",
       "        0.05959416,  0.00102265, -0.0491998 , -0.01705319, -0.08718075,\n",
       "        0.01625505, -0.02980288, -0.0290526 , -0.00953743,  0.05582019,\n",
       "       -0.01028011, -0.03645533, -0.01349658,  0.02922405,  0.00950451,\n",
       "        0.00265618, -0.04982678,  0.03399677, -0.01487433, -0.04223935,\n",
       "        0.04377802,  0.02429913, -0.0070756 , -0.03900617,  0.01343666,\n",
       "        0.02140767,  0.01819231, -0.01605036, -0.05664711, -0.04864142,\n",
       "        0.03207529,  0.02343914,  0.05991454, -0.05697872,  0.02682413,\n",
       "       -0.03659304,  0.00211397,  0.06056992,  0.0045533 ,  0.04212849,\n",
       "        0.02340636, -0.02793154, -0.03264666, -0.04004766,  0.00581279,\n",
       "       -0.03659985, -0.0064567 , -0.05559311,  0.06379316, -0.0078343 ,\n",
       "        0.01341806,  0.01203992,  0.03527417,  0.05531978, -0.00742858,\n",
       "        0.06247552,  0.03432331,  0.00841827,  0.03397123,  0.06450162,\n",
       "        0.06434911,  0.02801572, -0.04816007,  0.02539716, -0.01275051],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.get_vector(\"hotrec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tourism', 0.9997771382331848),\n",
       " ('sustainability', 0.9997443556785583),\n",
       " ('measure', 0.9997432231903076),\n",
       " ('plan', 0.9997410774230957),\n",
       " ('strategy', 0.9997352957725525),\n",
       " ('eu', 0.9997327327728271),\n",
       " ('travel', 0.9997319579124451),\n",
       " ('service', 0.9997313618659973),\n",
       " ('target', 0.9997299909591675),\n",
       " ('develop', 0.999729573726654)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = [\"sustainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "wv2 = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'hotrec' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wv2\u001b[39m.\u001b[39;49mmost_similar(positive\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mhotrec\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\ecaudron001\\Documents\\GitHub\\semic_pledges\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[39m=\u001b[39m item[\u001b[39m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[39m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_vector(keys, weight, pre_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, post_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ignore_missing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    842\u001b[0m all_keys \u001b[39m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, _KEY_TYPES) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ecaudron001\\Documents\\GitHub\\semic_pledges\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present in vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[39mif\u001b[39;00m total_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[39m=\u001b[39m mean \u001b[39m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'hotrec' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "wv2.most_similar(positive=[\"hotrec\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
