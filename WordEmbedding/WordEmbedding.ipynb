{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirPpath = Path(os.path.abspath('')).parent # Fetching the current directory path - Specific for ipynb file - For .py: Path(os.path.dirname(os.path.realpath(__file__)).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "PledgesCsvPath = str(DirPpath.absolute()) + \"/CleanedData.csv\"  \n",
    "\n",
    "PledgesDf = pd.read_csv(PledgesCsvPath, index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Achieve sustainable and flexible solutions for multimodal transport and develop policies to protect natural heritage and biodiversity, respecting the socio-cultural authenticity of host communities.  CNA Turismo e Commercio, over the three-year period (between autumn 2022 and throughout 2023), will organize training seminars for businesses - with the involvement of public and private stakeholders - aimed at the implementation of concrete solutions for the development of good practices for a supply of multimodal transport and protocols for the respect and protection of the natural heritage and biodiversity.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def first_clean(text):\n",
    "    return \" \".join(text.split()).replace(\"_x000D_\",\"\")\n",
    "\n",
    "first_clean(PledgesDf.iloc[3,1]) # Delete the returns from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'achieve sustainable and flexible solutions for multimodal transport and develop policies to protect natural heritage and biodiversity respecting the socio cultural authenticity of host communities cna turismo e commercio over the three year period between autumn and throughout will organize training seminars for businesses with the involvement of public and private stakeholders aimed at the implementation of concrete solutions for the development of good practices for a supply of multimodal transport and protocols for the respect and protection of the natural heritage and biodiversity'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower() # Lowercase all the characters from the string\n",
    "    text = text.strip() # Remove the leading and trailing whitespaces\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text) # Removing Punctuation\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text)) # Remove non alphanumeric characters\n",
    "    text = re.sub(r'\\d', '', text) # Removing digits\n",
    "    text = re.sub(r'\\s+', ' ', str(text).strip()) # Replacing \"double, triple, etc\" whitespaces by one\n",
    "    return text\n",
    "\n",
    "preprocess(first_clean(PledgesDf.iloc[3,1])) # Testing on pledge 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'achieve sustainable flexible solutions multimodal transport develop policies protect natural heritage biodiversity respecting socio cultural authenticity host communities cna turismo e commercio three year period autumn throughout organize training seminars businesses involvement public private stakeholders aimed implementation concrete solutions development good practices supply multimodal transport protocols respect protection natural heritage biodiversity'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stopword(string):\n",
    "    a = [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "stopword(preprocess(first_clean(PledgesDf.iloc[3,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'achieve sustainable flexible solution multimodal transport develop policy protect natural heritage biodiversity respect socio cultural authenticity host community cna turismo e commercio three year period autumn throughout organize training seminar business involvement public private stakeholder aim implementation concrete solution development good practice supply multimodal transport protocol respect protection natural heritage biodiversity'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(nltk.word_tokenize(string))  # Get position tags\n",
    "    a = [wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in\n",
    "         enumerate(word_pos_tags)]  # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "lemmatizer(stopword(preprocess(first_clean(PledgesDf.iloc[3,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "\n",
    "    global n\n",
    "    n = n +1\n",
    "    \n",
    "    print(\"**************\")\n",
    "    print(\"n is : \")\n",
    "    print(n)\n",
    "    print(\"length of the text is : \")\n",
    "    print(len(first_clean(text)))\n",
    "    return lemmatizer(stopword(preprocess(first_clean(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "print(\"begin pre-processing\")\n",
    "PledgesDf['clean_text'] = PledgesDf['Pledge'].apply(lambda x: preprocessing(x))\n",
    "print(\"end pre-processing\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\" \".join(PledgesDf['clean_text']).split()).value_counts()[:30].index.tolist() # Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords2 = pd.Series(\" \".join(PledgesDf['clean_text']).split()).value_counts()[:30].index.tolist()\n",
    "\n",
    "def RemoveFrequentWords(string, FrequentWords):\n",
    "    a = [i for i in string.split() if i not in FrequentWords] # Removing usual english stopwords from the string\n",
    "    return ' '.join(a) #Output - Same string after all the transformations\n",
    "\n",
    "PledgesDf['clean_text'] = PledgesDf['clean_text'].apply(lambda x: RemoveFrequentWords(x, StopWords2))\n",
    "\n",
    "print(PledgesDf.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(i) for i in PledgesDf[\"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3237"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = {}\n",
    "for sent in tokens:\n",
    "    for i in sent:\n",
    "\n",
    "        if i not in word_freq.keys():\n",
    "            word_freq[i] = 1\n",
    "        else:\n",
    "            word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(PledgesDf[\"clean_text\"])\n",
    "\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names_out()\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)\n",
    "\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2 = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Word2Vec model with Average method\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "            return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "w2v2 = dict(zip(wv2.index_to_key, wv2.vectors))\n",
    "modelw2 = MeanEmbeddingVectorizer(w2v2)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "vectors_w2v2 = modelw2.transform(tokens)\n",
    "\n",
    "print(vectors_w2v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocIndexV1 = pd.DataFrame(vectors_w2v2)\n",
    "IndexedPath = str(DirPpath.absolute()) + \"\\IndexedDataV1.csv\"\n",
    "DocIndexV1.to_csv(IndexedPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Word2Vec model with TF-IDF method\n",
    "class TfIdfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "            return self\n",
    "\n",
    "    def transform(self, X, tfidf):\n",
    "\n",
    "        DocList = []\n",
    "        i = 0\n",
    "        \n",
    "        for words in X:\n",
    "\n",
    "            WordList = []\n",
    "\n",
    "            for w in words:\n",
    "                 \n",
    "                try:\n",
    "                    if w in self.word2vec:\n",
    "                        weight = tfidf[w].iloc[i]\n",
    "                        WordList.append(self.word2vec[w] * weight)\n",
    "                    else:\n",
    "                        WordList.append(np.zeros(self.dim))\n",
    "                except:\n",
    "                    WordList.append(np.zeros(self.dim))\n",
    "\n",
    "            i+=1\n",
    "            DocList.append(np.sum(np.array(WordList), axis = 0))\n",
    "        \n",
    "        return np.array(DocList)\n",
    "\n",
    "\n",
    "w2v2 = dict(zip(wv2.index_to_key, wv2.vectors))\n",
    "modelw2 = TfIdfEmbeddingVectorizer(w2v2)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "vectors_w2v2 = modelw2.transform(tokens, df_tfidfvect)\n",
    "\n",
    "print(vectors_w2v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocIndexV1 = pd.DataFrame(vectors_w2v2)\n",
    "IndexedPath = str(DirPpath.absolute()) + \"\\IndexedDataV1Tf.csv\"\n",
    "DocIndexV1.to_csv(IndexedPath)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Google 300 news Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Setting the model with its hyperparameters\n",
    "#model = Word2Vec(sentences = tokens, vector_size=300, min_count = 1)\n",
    "model2 = Word2Vec(sentences = tokens, vector_size=300, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(r\"C:\\Users\\ecaudron001\\Downloads\\GoogleNews-vectors-negative300.bin\",\n",
    "                                         binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 923 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m total_examples \u001b[39m=\u001b[39m model2\u001b[39m.\u001b[39mcorpus_count\n\u001b[0;32m      3\u001b[0m model2\u001b[39m.\u001b[39mbuild_vocab([\u001b[39mlist\u001b[39m(model2\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mkey_to_index\u001b[39m.\u001b[39mkeys())], update\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m model2\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mintersect_word2vec_format(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mecaudron001\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDownloads\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mGoogleNews-vectors-negative300.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m, binary\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      5\u001b[0m model2\u001b[39m.\u001b[39mtrain(tokens, total_examples\u001b[39m=\u001b[39mtotal_examples, epochs\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39miter)\n",
      "File \u001b[1;32mc:\\Users\\ecaudron001\\Documents\\GitHub\\semic_pledges\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1771\u001b[0m, in \u001b[0;36mKeyedVectors.intersect_word2vec_format\u001b[1;34m(self, fname, lockf, binary, encoding, unicode_errors)\u001b[0m\n\u001b[0;32m   1769\u001b[0m             overlap_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1770\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectors[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(word)] \u001b[39m=\u001b[39m weights\n\u001b[1;32m-> 1771\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectors_lockf[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(word)] \u001b[39m=\u001b[39m lockf  \u001b[39m# lock-factor: 0.0=no changes\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39mfor\u001b[39;00m line_no, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(fin):\n",
      "\u001b[1;31mIndexError\u001b[0m: index 923 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "model2.build_vocab(tokens)\n",
    "total_examples = model2.corpus_count\n",
    "model2.build_vocab([list(model2.wv.key_to_index.keys())], update=True)\n",
    "model2.wv.intersect_word2vec_format(r\"C:\\Users\\ecaudron001\\Downloads\\GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "model2.train(tokens, total_examples=total_examples, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
